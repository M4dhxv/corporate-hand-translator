# Corporate Signal Translator ğŸ–ï¸ğŸ’¼

> *Translate your hand gestures into peak corporate jargon â€” powered by AI, running entirely in your browser.*

[![Live Demo](https://img.shields.io/badge/demo-live-brightgreen?style=flat-square)](https://corporate-hand-translator.vercel.app)
[![Version](https://img.shields.io/badge/version-3.2.0-blue?style=flat-square)](#changelog)
[![License](https://img.shields.io/badge/license-MIT-green?style=flat-square)](LICENSE)
[![TensorFlow.js](https://img.shields.io/badge/TensorFlow.js-v4.17-orange?style=flat-square&logo=tensorflow)](https://www.tensorflow.org/js)
[![Vercel](https://img.shields.io/badge/deployed%20on-Vercel-black?style=flat-square&logo=vercel)](https://vercel.com)

---

## âœ¨ What Is This?

A satirical web app that uses **real-time hand tracking** and a **TensorFlow.js neural network** to detect your gestures and translate them into the finest corporate speak. No backend. No API keys. Just pure, client-side machine learning.

Show your hand to the camera â†’ AI detects the gesture â†’ You get a corporate power phrase.

---

## ğŸ¯ Features

| Feature | Description |
|---------|-------------|
| ğŸ§  **ML Gesture Recognition** | TensorFlow.js neural network classifies gestures in real-time |
| ğŸ“ **Training Mode** | Personalize gesture recognition in-browser â€” no servers, no uploads |
| ğŸ¥ **Hand Tracking** | MediaPipe Hands detects 21 landmarks per hand at 30fps |
| ğŸ¦´ **Skeleton Overlay** | Live hand skeleton visualization on the video feed |
| ğŸ”Š **Text-to-Speech** | Reads corporate phrases aloud (toggleable) |
| ğŸ“± **Responsive Design** | Glassmorphism UI that works on desktop and mobile |
| â˜ï¸ **Zero Backend** | Everything runs in the browser â€” deploy as static files |

---

## ğŸ¤ Gesture Translations

| Gesture | Emoji | Corporate Translation |
|---------|-------|-----------------------|
| Open Palm | âœ‹ | *"Let's put a pin in that for now."* |
| Closed Fist | âœŠ | *"We need to circle back to the core deliverables."* |
| Thumbs Up | ğŸ‘ | *"I am fully aligned with this initiative."* |
| Pointing Up | â˜ï¸ | *"Let's take this offline."* |
| Peace Sign | âœŒï¸ | *"We have verified the cross-functional synergy."* |

---

## ğŸ—ï¸ Architecture

```
Webcam â†’ MediaPipe Hands â†’ 21 Landmarks (x,y,z)
    â†“
Preprocessing â†’ 63-float vector (normalized to wrist)
    â†“
TF.js Neural Network (63 â†’ 128 â†’ 64 â†’ 5)
    â†“
Gesture Label + Confidence Score
    â†“
Corporate Phrase â†’ UI + TTS

â”€â”€â”€ Training Mode â”€â”€â”€
Record Gestures â†’ Collect Landmarks â†’ Train in Browser
    â†“
Save to IndexedDB â†’ Auto-load on next visit
```

### ML Model Details

| Property | Value |
|----------|-------|
| Input shape | `[1, 63]` â€” 21 landmarks Ã— 3 coordinates |
| Architecture | Dense(128, ReLU) â†’ Dropout(0.3) â†’ Dense(64, ReLU) â†’ Dropout(0.2) â†’ Dense(5, Softmax) |
| Parameters | 16,773 |
| Model size | ~67 KB |
| Training accuracy | 100% |
| Confidence threshold | 0.65 |
| Inference time | < 5ms per frame |

---

## ï¿½ï¸ Gesture Decision Engine (v3.2+)

**The problem**: Raw ML predictions jitter frame-to-frame, causing false gesture triggers and repeated TTS.

**The solution**: A deterministic decision layer that hardens real-world reliability without retraining.

### What It Does

| Problem | Solution | Benefit |
|---------|----------|---------|
| CLOSED_FIST misclassified as THUMBS_UP | **Thumb Dominance Gate** â€” requires thumb 30% more extended than other fingers | No more false "I am fully aligned" when making a fist |
| Camera jitter flips between gestures | **Stability Voting** â€” requires 8 consecutive frames of same gesture | Smooth, intentional-feeling gestures |
| Same gesture triggers TTS repeatedly | **Intent Lock** â€” 2.5s cooldown after acceptance | Corporate phrases don't spam |

### Configuration

Located in `src/ml/gestureDecisionEngine.js`:

```javascript
STABILITY_FRAMES = 8              // Frames to stabilize (higher = stricter)
GESTURE_COOLDOWN_MS = 2500        // Cooldown after acceptance
THUMB_DOMINANCE_THRESHOLD = 1.3   // Thumb extension factor (higher = stricter)
CONFIDENCE_TIE_BREAK_THRESHOLD = 0.1  // Confidence tie-breaker margin
```

### For Developers

Debug gesture decisions in the browser console:

```javascript
import { getEngineState } from './src/ml/gestureDecisionEngine';

// Check engine state at any time
console.log(getEngineState());
// Output: {
//   acceptedGesture: 'THUMBS_UP',
//   inCooldown: true,
//   stabilityBufferSize: 8,
//   cooldownTimeRemaining: 1500
// }
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| **Framework** | React 18 + Vite 5 |
| **Styling** | Tailwind CSS 3 |
| **Hand Tracking** | MediaPipe Hands (CDN) |
| **ML Inference** | TensorFlow.js 4.17 |
| **Decision Logic** | Gesture Decision Engine (in-browser) |
| **Speech** | Web Speech API (native) |
| **Hosting** | Vercel (static) |

---

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** 18+ and **npm**
- A **webcam** (built-in or external)
- A modern browser (Chrome, Edge, Firefox)

### Installation

```bash
# Clone the repository
git clone https://github.com/M4dhxv/corporate-hand-translator.git
cd corporate-hand-translator

# Install dependencies
npm install

# Start development server
npm run dev
```

Open [http://localhost:5173](http://localhost:5173) and allow camera access.

### Building for Production

```bash
npm run build
npm run preview  # Preview the production build locally
```

---

## ğŸ“ Personalize Gestures (Training Mode)

Users can train their own gesture model **entirely in the browser** â€” no servers, no uploads, no accounts.

### How It Works

1. Click **"Personalize Gestures"** in the header
2. Hold each gesture steady in front of the camera
3. Click the **Record** button for each gesture (auto-captures ~30 frames in 3 seconds)
4. Collect at least **10 samples per gesture**
5. Click **"Train My Gestures"** â€” training runs in-browser (~15 seconds)
6. Done! Your personalized model is saved to **IndexedDB** and loads automatically on future visits

### Technical Details

| Property | Value |
|----------|-------|
| Storage | `indexeddb://corporate-gesture-model` |
| Model size | < 500 KB |
| Training epochs | 50 |
| Privacy | All data stays in your browser |
| Reset | Click "Reset Personalization" to revert to the default model |

---

## ğŸ§  Default Model Training

The default ML model is pre-trained and included in the repo (`public/model/`). To retrain:

```bash
# Generate synthetic data + train the neural network
npm run train-model
```

This runs `scripts/trainModel.mjs`, which:
1. Generates 4,000 synthetic hand landmark samples (800 per class)
2. Trains a feed-forward neural network for 100 epochs
3. Saves the model to `public/model/` as static assets

> The training script uses synthetic data based on realistic hand poses. No external datasets or GPU required.

---

## ğŸ“ Project Structure

```
corporate-hand-translator/
â”œâ”€â”€ public/
â”‚   â””â”€â”€ model/                     # Default TF.js model (static assets)
â”‚       â”œâ”€â”€ model.json             # Model topology + weights manifest
â”‚       â””â”€â”€ group1-shard1of1.bin   # Model weights
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ trainModel.mjs             # Offline model training script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ gestureModel.js        # Model loader + inference engine
â”‚   â”‚   â”œâ”€â”€ localModelManager.js   # IndexedDB model persistence
â”‚   â”‚   â””â”€â”€ gestureTrainer.js      # In-browser training pipeline
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â””â”€â”€ useHandTracking.js     # MediaPipe + ML integration hook
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ VideoFeed.jsx          # Camera feed + canvas overlay
â”‚   â”‚   â”œâ”€â”€ PhraseOverlay.jsx      # Gesture phrase display
â”‚   â”‚   â””â”€â”€ TrainingMode.jsx       # Training Mode UI panel
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ gestureClassifier.js   # Legacy rule-based classifier
â”‚   â”œâ”€â”€ App.jsx                    # Main application component
â”‚   â”œâ”€â”€ index.css                  # Global styles + design system
â”‚   â””â”€â”€ main.jsx                   # Application entry point
â”œâ”€â”€ package.json
â”œâ”€â”€ vite.config.js
â”œâ”€â”€ tailwind.config.js
â”œâ”€â”€ vercel.json
â”œâ”€â”€ CHANGELOG.md
â””â”€â”€ LICENSE
```

---

## â˜ï¸ Deployment

### Vercel (Recommended)

The app is designed for Vercel's static hosting. Just push to GitHub â€” Vercel auto-deploys.

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/M4dhxv/corporate-hand-translator)

### Any Static Host

```bash
npm run build
# Upload the `dist/` folder to any static host (Netlify, GitHub Pages, etc.)
```

> **Note:** The ML model files in `public/model/` are automatically copied to `dist/model/` during build and served as static assets.

---

## ğŸ”’ Privacy & Security

- **All processing is client-side** â€” video frames and landmarks never leave your browser
- **No data collection** â€” zero telemetry, zero analytics
- **No API keys** â€” everything is open-source and self-contained
- **Camera access** is only used for hand tracking and is not recorded

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-gesture`)
3. Commit your changes (`git commit -m 'Add amazing gesture'`)
4. Push to the branch (`git push origin feature/amazing-gesture`)
5. Open a Pull Request

### Adding New Gestures

1. Add a new generator function in `scripts/trainModel.mjs`
2. Add the class to `GESTURE_CLASSES` array
3. Run `npm run train-model` to retrain
4. Add phrase mapping in `src/ml/gestureModel.js`

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [MediaPipe](https://mediapipe.dev/) by Google for hand landmark detection
- [TensorFlow.js](https://www.tensorflow.org/js) for browser-based ML inference
- [Vite](https://vitejs.dev/) for blazing-fast development
- [Tailwind CSS](https://tailwindcss.com/) for utility-first styling

---

<p align="center">
  <em>Built with â¤ï¸ and excessive corporate synergy</em>
</p>
